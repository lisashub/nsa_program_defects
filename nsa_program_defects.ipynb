{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e49d7c",
   "metadata": {},
   "source": [
    "# NASA Program Defects:  Classifier Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda86d07",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "When representing real-world scenarios within machine learning models, it is important to know how to compare performance across model architectures. From these comparisons, a data scientist is able to identify the strongest match for problem representation. Additionally, it is important for data scientists to know how fine-tune models within a given architecture both for cross-method comparison and inter-method comparison.\n",
    "\n",
    "This small project generates five different classifier model architectures that predict the presence of software defects in a given NASA software program. The project generates 10 models for each method using 10-fold cross-validation. The project then captures Area Under Curve scores for cross-method performance comparison. For two of the five models, a given paremeter is fine-tune prior to final cross-validation training and testing.\n",
    "\n",
    "Technical Note: For the cross-validation process, the parameter “n_jobs” is adjusted so that all but one available system CPU is dedicated to completing the model_selection.cross-validate() call; this is done to improve computing performance.\n",
    "\n",
    "\n",
    "## Brief Description of the Data\n",
    "\n",
    "The selected dataset – named “pc4” - is part of the National Aeronautics and Space Administration (NASA) program defect datasets. The set’s data describes earth-orbiting satellite flight software. The datapoints were compiled by Shirabad & Menzies in 2015[1]. For this project, the data is directly imported from OpenML's database located at: https://www.openml.org/search?type=data&status=active&id=1049\n",
    "\n",
    "The set examples consist of 37 numerical descriptive features and one Boolean target feature which describes whether an example program contained one or more defects. The descriptive features are based on McCabe and Halstead metrics for software complexity. The dataset contains 1458 program instances and no missing values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d766cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import tree\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81c9ff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess available system CPU for cross-validation performance tuning\n",
    "import os\n",
    "n_cpu = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c3d334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*********Default Decision Tree AUC Scoring**********\n",
      "AUC for each model:  [0.70703125 0.63541667 0.72005208 0.72265625 0.69921875 0.84201389\n",
      " 0.69487847 0.67881944 0.85110294 0.63924632]\n",
      "Mean AUC:  0.7190436070261439\n",
      "\n",
      "\n",
      "*********Tuned Decision Tree AUC Scoring**********\n",
      "AUC for each model:  [0.90342882 0.94639757 0.85872396 0.9296875  0.87521701 0.9047309\n",
      " 0.92578125 0.91232639 0.88671875 0.88304228]\n",
      "Mean AUC:  0.9026054432189543\n",
      "Best \"min_sample_leaf\" parameter value:  40\n",
      "\n",
      "\n",
      "*********Tuned Random Forest Ensemble AUC Scoring**********\n",
      "AUC for each model:  [0.94704861 0.93012153 0.93055556 0.96419271 0.95138889 0.94618056\n",
      " 0.94335938 0.94921875 0.95036765 0.93474265]\n",
      "Mean AUC:  0.944717626633987\n",
      "Best \"n_estimators\" parameter value:  500\n",
      "\n",
      "\n",
      "*********Default Bagged Ensemble AUC Scoring**********\n",
      "AUC for each model:  [0.90082465 0.92599826 0.89583333 0.94661458 0.94314236 0.90147569\n",
      " 0.89322917 0.91341146 0.91130515 0.94462316]\n",
      "Mean AUC:  0.9176457822712418\n",
      "\n",
      "\n",
      "*********Default AdaBoost Ensemble AUC Scoring**********\n",
      "AUC for each model:  [0.89756944 0.94661458 0.93402778 0.89149306 0.87196181 0.93988715\n",
      " 0.87782118 0.88628472 0.93106618 0.93520221]\n",
      "Mean AUC:  0.9111928104575163\n"
     ]
    }
   ],
   "source": [
    "#Select NSA Dataset from OpenML\n",
    "nsa_dataset = datasets.fetch_openml(data_id=1049)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "#Run Model Generation & Evaluation\n",
    "print(\"*********Default Decision Tree AUC Scoring**********\")\n",
    "dtc_default = tree.DecisionTreeClassifier() #default parameters\n",
    "cv_default = model_selection.cross_validate(dtc_default, nsa_dataset.data, nsa_dataset.target, scoring = \"roc_auc\", cv = 10, n_jobs = n_cpu-1)\n",
    "print(\"AUC for each model: \", cv_default[\"test_score\"])\n",
    "print(\"Mean AUC: \", cv_default[\"test_score\"].mean())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"*********Tuned Decision Tree AUC Scoring**********\")\n",
    "parameters = [{\"min_samples_leaf\":[2,4,6,8,10,20,30,40,50]}]\n",
    "dtc_tuned = model_selection.GridSearchCV(dtc_default, parameters, scoring = \"roc_auc\", cv = 10, n_jobs = n_cpu-1 )\n",
    "cv_dtc_tuned = model_selection.cross_validate(dtc_tuned, nsa_dataset.data, nsa_dataset.target, scoring = \"roc_auc\", cv = 10, n_jobs = n_cpu-1)\n",
    "dtc_tuned.fit(nsa_dataset.data, nsa_dataset.target)\n",
    "print(\"AUC for each model: \", cv_dtc_tuned[\"test_score\"])\n",
    "print(\"Mean AUC: \", cv_dtc_tuned[\"test_score\"].mean())\n",
    "print(\"Best \\\"min_sample_leaf\\\" parameter value: \" , dtc_tuned.best_params_['min_samples_leaf'])\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"*********Tuned Random Forest Ensemble AUC Scoring**********\")\n",
    "parameters = [{\"n_estimators\": [50, 100, 500]}]\n",
    "rf = RandomForestClassifier()\n",
    "rf_tuned = model_selection.GridSearchCV(rf, parameters, scoring = \"roc_auc\", cv = 10, n_jobs = n_cpu-1)\n",
    "cv_rf_tuned = model_selection.cross_validate(rf_tuned, nsa_dataset.data, nsa_dataset.target, scoring=\"roc_auc\", cv=10, n_jobs = n_cpu-1)\n",
    "rf_tuned.fit(nsa_dataset.data,nsa_dataset.target)\n",
    "print(\"AUC for each model: \", cv_rf_tuned[\"test_score\"])\n",
    "print(\"Mean AUC: \", cv_rf_tuned[\"test_score\"].mean())\n",
    "print(\"Best \\\"n_estimators\\\" parameter value: \", rf_tuned.best_params_['n_estimators'])\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"*********Default Bagged Ensemble AUC Scoring**********\")\n",
    "dtc_bagged = BaggingClassifier()\n",
    "cv_bagged = model_selection.cross_validate(dtc_bagged, nsa_dataset.data, nsa_dataset.target, scoring = \"roc_auc\", cv = 10, n_jobs = n_cpu-1)\n",
    "print(\"AUC for each model: \", cv_bagged[\"test_score\"])\n",
    "print(\"Mean AUC: \", cv_bagged[\"test_score\"].mean())\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"*********Default AdaBoost Ensemble AUC Scoring**********\")\n",
    "dtc_adaboost = AdaBoostClassifier()\n",
    "cv_adaboost = model_selection.cross_validate(dtc_adaboost, nsa_dataset.data, nsa_dataset.target, scoring = \"roc_auc\", cv = 10, n_jobs = n_cpu-1)\n",
    "print(\"AUC for each model: \", cv_adaboost[\"test_score\"])\n",
    "print(\"Mean AUC: \", cv_adaboost[\"test_score\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc94a3cc",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "The default and tuned single decision trees performed the least competitively compared to the ensemble models with AUC scores of .7252 and .9015, respectively. That these single tree models were generally less competitive compared to the ensemble models aligns with the assertion that ensemble models generally provide better predictions due to their inter-diversity, independence, and ability to generate unique language/search biases that a single model may not be able to produce.\n",
    "\n",
    "The tuned random forest ensemble model was the most competitive predictor, producing from its 10-fold cross-validation a mean AUC score of 0.9450. The number of trees (“n_estimators”) for each random forest ensemble was notably optimized to 500 trees using GridSearchCV() – this was  from an available parameter grid of 100, 300, and 500 trees. From this, it seems that an increase of decision trees generated within a random forest positively correlates with improved performance; attempting to increase the number of trees may continue to improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a77d713",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "[1] Sayyad Shirabad, J. and Menzies, T.J. (2005) The PROMISE Repository of Software Engineering Databases. School of Information Technology and Engineering, University of Ottawa, Canada.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d11131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
